<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Plutosoft delimited - misc</title><link href="https://pluteski.github.io/speech-to-text/" rel="alternate"></link><link href="https://pluteski.github.io/speech-to-text/feeds/misc.atom.xml" rel="self"></link><id>https://pluteski.github.io/speech-to-text/</id><updated>2018-01-07T00:00:00-08:00</updated><entry><title>Gossiping about Hashgraph</title><link href="https://pluteski.github.io/speech-to-text/gossiping-about-hashgraph.html" rel="alternate"></link><published>2018-01-07T00:00:00-08:00</published><updated>2018-01-07T00:00:00-08:00</updated><author><name>Mark Plutowski</name></author><id>tag:pluteski.github.io,2018-01-07:/speech-to-text/gossiping-about-hashgraph.html</id><summary type="html">&lt;h1&gt;Dishing the dirt on Hashgraph&lt;/h1&gt;
&lt;p&gt;&lt;img alt="emonocle byEMIIA" src="https://1.bp.blogspot.com/-DpZwufJmu_Y/Wh6lGuTNRmI/AAAAAAAABUU/0W2PLj5w-j4ql9RE8Otwk5DrB3UcgKOGQCLcBGAs/s1600/hashgraph%2B%25282%2529.gif"&gt;
&lt;p style="text-align: right;"&gt;&lt;em&gt;(emonocle byEMIIA)&lt;/em&gt;&lt;/p&gt;&lt;/p&gt;
&lt;h2&gt;&lt;em&gt;pssst!&lt;/em&gt; ... &lt;em&gt;have you heard about Hashgraph?&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://hashgraph.com"&gt;Hashgraph&lt;/a&gt;
is a data structure which together with
the &lt;a href="http://www.swirlds.com/downloads/SWIRLDS-TR-2016-01.pdf"&gt;Swirlds distributed consensus algorithm&lt;/a&gt;
provides &lt;a href="https://hashgraph.com/faq/#what-is-bft"&gt;asynchronous byzantine fault tolerance&lt;/a&gt;, using what it calls "gossip about gossip."&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Whaaaat?&lt;/em&gt;  A system based on second hand rumors?  Whisper logs? We're really …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Dishing the dirt on Hashgraph&lt;/h1&gt;
&lt;p&gt;&lt;img alt="emonocle byEMIIA" src="https://1.bp.blogspot.com/-DpZwufJmu_Y/Wh6lGuTNRmI/AAAAAAAABUU/0W2PLj5w-j4ql9RE8Otwk5DrB3UcgKOGQCLcBGAs/s1600/hashgraph%2B%25282%2529.gif"&gt;
&lt;p style="text-align: right;"&gt;&lt;em&gt;(emonocle byEMIIA)&lt;/em&gt;&lt;/p&gt;&lt;/p&gt;
&lt;h2&gt;&lt;em&gt;pssst!&lt;/em&gt; ... &lt;em&gt;have you heard about Hashgraph?&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://hashgraph.com"&gt;Hashgraph&lt;/a&gt;
is a data structure which together with
the &lt;a href="http://www.swirlds.com/downloads/SWIRLDS-TR-2016-01.pdf"&gt;Swirlds distributed consensus algorithm&lt;/a&gt;
provides &lt;a href="https://hashgraph.com/faq/#what-is-bft"&gt;asynchronous byzantine fault tolerance&lt;/a&gt;, using what it calls "gossip about gossip."&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Whaaaat?&lt;/em&gt;  A system based on second hand rumors?  Whisper logs? We're really supposed to trust this thing?
Read on to find out.  &lt;em&gt;At least about what I know.  If you know something I don't, you can tell someone in our mutual hashgraph.
If we live in a world defined by gossip, as is rumored to be the case, the math says that I'll almost surely find out.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;How does Hashgraph compare to Bitcoin?&lt;/h2&gt;
&lt;p&gt;Distributed consensus is an extremely useful concept in distributed computing.
For decades it was doable only among a small number of computers ("nodes").
Nakamoto's Bitcoin cryptocurrency  &lt;a href="http://vukolic.com/iNetSec_2015.pdf"&gt;presented a new way to achieve scalable decentralized consensus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Bitcoin protocol does not implement consensus
in the traditional distributed computing sense.
&lt;a href="http://vukolic.com/iNetSec_2015.pdf"&gt;Instead it achieves consensus via probabilistic agreement&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A primary goal of a cryptocurrency is to &lt;a href="http://mathworld.wolfram.com/TotallyOrderedSet.html"&gt;totally order&lt;/a&gt; transactions
on a &lt;a href="https://www.investopedia.com/terms/d/distributed-ledgers.asp"&gt;distributed ledger&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hashgraph also provides a total order on a distributed transaction ledger,
but does so using a &lt;a href="https://steemit.com/steemit/@decryptson/hashgraph"&gt;different approach&lt;/a&gt;.
Whereas the Bitcoin network builds up its transaction history in the form of a “blockchain”,
adding &lt;a href="https://bitcoinmagazine.com/articles/selfish-mining-a-25-attack-against-the-bitcoin-network-1383578440/"&gt;a new block on top of the previous block every ten minutes&lt;/a&gt;,
Hashgraph grows a time directed acyclic graph akin to a braided forest of trees
using &lt;a href="https://hashgraph.com/faq/#how-does-it-work"&gt;"virtual voting" and "gossip about gossip"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Despite Bitcoin demonstrating tremendous value, its blockchain plus Proof-of-Work (PoW) approach
presents several pitfalls.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it is extremely wasteful since &lt;a href="http://www.nasdaq.com/article/byzantine-fault-tolerance-the-key-for-blockchains-cm810058"&gt;by design&lt;/a&gt; PoW expends huge amounts of computing power.&lt;/li&gt;
&lt;li&gt;it is slow, limited to tens of transactions per second&lt;/li&gt;
&lt;li&gt;it allows a huge backlog of unconfirmed transactions to accumulate&lt;/li&gt;
&lt;li&gt;it requires a lot of network bandwidth&lt;/li&gt;
&lt;li&gt;is &lt;a href="https://arxiv.org/abs/1311.0243"&gt;susceptible to a 25% economic attack&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Virtual elections. Better than real ones?&lt;/h2&gt;
&lt;p&gt;Hashgraph is comparable to PBFT, Paxos, Raft, Zab, and other consensus seeking systems that rely on leaders and voting schemes.
But hashgraph eschews the comparison, claiming that it doesn't even use voting.
It comes to consensus about what happened, and when,
by cleverly tallying highly compressed event logs
based on "famous witnesses" that &lt;a href="https://www.swirlds.com/downloads/SWIRLDS-TR-2016-02.pdf"&gt;"strongly see"&lt;/a&gt; events.&lt;/p&gt;
&lt;p&gt;If participants have a bad clock or doctor their own timestamp,
the actual recorded timestamp will be the median of all the timestamps observed by credible witnesses.
If the exact timestamp is unnecessary and rank order is what is most important,
because the rank order based on median is fairly robust to extreme outliers,
the total ordering is less affected.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://tech.mit.edu/V123/N8/8voting.8n.html"&gt;Arrow's Theorem&lt;/a&gt; proves that no voting system is fair.
I'm no &lt;a href="https://www.princeton.edu/~cuff/voting/theory.html"&gt;voting theorist&lt;/a&gt;,
but in my (extremely humble) opinion, hashgraph comes pretty close.
It makes one want to believe in democracy again. Real-time continuous nationwide elections anyone?&lt;/p&gt;
&lt;h2&gt;Hashgraph: the good&lt;/h2&gt;
&lt;p&gt;It is &lt;a href="https://hackernoon.com/demystifying-hashgraph-benefits-and-challenges-d605e5c0cee5"&gt;fast&lt;/a&gt;,
&lt;a href="https://hashgraph.com/faq/#what-is-fairness"&gt;fair&lt;/a&gt;,
and &lt;a href="https://hashgraph.com/faq/#preventing-sybil-attacks"&gt;secure&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It promises total ordering of events.&lt;/li&gt;
&lt;li&gt;It promises strong &lt;a href="http://the-paper-trail.org/blog/barbara-liskovs-turing-award-and-byzantine-fault-tolerance/"&gt;Byzantine Fault Tolerance&lt;/a&gt; (BFT),
the gold standard of industrial grade distributed consensus.&lt;/li&gt;
&lt;li&gt;Requires minimal network bandwidth, because it passes things only around once.&lt;/li&gt;
&lt;li&gt;It does not require Proof-of-Work, so it does not require unnecessary computation.&lt;/li&gt;
&lt;li&gt;It is proven to quickly reach 100% certainty on the order of transactions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because it is fast and requires low network bandwidth, it can be used as a distributed memory system.
Because it provides a total order on transactions, it can be used as a multi-master database.&lt;/p&gt;
&lt;p&gt;Before we discuss its tx/sec (transactions/second), let's review the tx/sec of other distributed ledger technology (DLT) :
&lt;em&gt; Proof-of-Work Blockchain (Etherium, Bitcoin) : under 10 tx/sec
&lt;/em&gt; Paypal : &lt;a href="http://www.altcointoday.com/bitcoin-ethereum-vs-visa-paypal-transactions-per-second/"&gt;~200 tx/sec&lt;/a&gt;
* Visa : &lt;a href="https://mybroadband.co.za/news/security/190348-visanet-handling-100000-transactions-per-minute.html"&gt;~2K tx/sec&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Visa claims that its system has capacity to handle upwards of 50K tx/sec.&lt;/p&gt;
&lt;p&gt;Hashgraph's inventor says it can attain &lt;a href="https://www.hiddenforcespod.com/leemon-baird-hashgraph-distributed-ledger-technology-blockchain/"&gt;250K+ tx/sec&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To be fair, because of its positioning and licensing, hashgraph is most directly comparable to Hyperledger, which is
also a scalable DFT using a Practical Byzantine Fault Tolerance (PBFT) consensus algorithm.
Other enterprise-grade commercial systems include
LMAX and its variants, such as &lt;a href="http://lmax-exchange.github.io/disruptor/"&gt;LMAX Disruptor&lt;/a&gt;
and Bitshares.  Their ts/sec are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.hyperledger.org/about"&gt;Hyperledger&lt;/a&gt; : &lt;a href="https://www.altoros.com/blog/hyperledgers-sawtooth-lake-aims-at-a-thousand-transactions-per-second/"&gt;~1K tx/sec&lt;/a&gt; to &lt;a href="https://medium.com/chain-cloud-company-blog/hyperledger-vs-corda-pt-1-3723c4fa5028"&gt;~10K tx/sec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bitshares : &lt;a href="https://bitshares.org/technology/industrial-performance-and-scalability/"&gt;100K tx/sec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LMAX : &lt;a href="https://qconsf.com/sf2010/sf2010/presentation/LMAX+-+How+to+do+over+100K+concurrent+transactions+per+second+at+less+than+1ms+latency.html"&gt;100K tx/sec (2010)&lt;/a&gt; to &lt;a href="https://martinfowler.com/articles/lmax.html"&gt;6M tx/sec&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The bad&lt;/h2&gt;
&lt;p&gt;So what is the catch?&lt;/p&gt;
&lt;p&gt;Hashgraph is only deployed in &lt;a href="https://hackernoon.com/demystifying-hashgraph-benefits-and-challenges-d605e5c0cee5"&gt;private, permissioned-based networks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Whether it can be adapted to a truly decentralised public ledger remains to be seen, because it assumes that a node can determine :
&lt;em&gt; addresses of random nodes in the network for messaging, and
&lt;/em&gt; the number of other nodes N in the network&lt;/p&gt;
&lt;p&gt;This is because the algorithm requires a node to be able to (a) pick another node at random, and (b) know the value of ⅔ * N.&lt;/p&gt;
&lt;p&gt;It needs a means of registering and unregistering of members in the network,
whereas public blockchains allow nodes to sign in and out to the network without any notice.
It is sometimes called a Permissioned Blockchain because
&lt;a href="https://hashgraph.com/faq/#is-there-a-cryptocurrency"&gt;there is no hashgraph public ledger or cryptocurrency and is currently only implemented on permissioned networks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally, the following seem to be a major showstopper for many members of the cryptocurrency development community:
&lt;em&gt; It requires a license to use.
&lt;/em&gt; It is patented (US Patents #&lt;a href="http://www.leemon.com/papers/2017b.pdf"&gt;9,646,029&lt;/a&gt;, #&lt;a href="http://www.leemon.com/papers/2016b4.pdf"&gt;9,529,923&lt;/a&gt;, #&lt;a href="http://www.leemon.com/papers/2016b3.pdf"&gt;9,390,154&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;About the author&lt;/h4&gt;
&lt;p&gt;I am just an interested learner coming up to speed on blockchain technologies, distributed ledger technologies,
and decentralized consensus systems.
At the time of writing this article I have no investment in any cryptocurrency, I do not work for or represent
any organization related to cryptocurrencies or distributed ledger technologies.
As I am still learning about this area, I am not an expert.&lt;/p&gt;
&lt;p&gt;So why am I writing this?&lt;/p&gt;
&lt;p&gt;Hashgraph impressed on me the value of &lt;a href="https://hashgraph.com/faq/#how-does-it-work"&gt;gossip&lt;/a&gt;.
Instead of waiting to share your opinion until you are absolutely certain of what you know, a
"gossip protocol" says to share your observations early and often. So, here are mine about hashgraph.
Hashgraph mathematically proves that
gossip can allow decentralized participants to rapidly share what they
have observed (as well as what they have heard second-hand from others),
and rapidly agree on what is to be believed. Who am I to argue with math?&lt;/p&gt;
&lt;h2&gt;Concluding remark&lt;/h2&gt;
&lt;p&gt;DYOR! Consider this to be informed gossip. But do share yours with me too. The math says to, so, yeah just do what the math says.&lt;/p&gt;
&lt;p&gt;&lt;img alt="tommytwohats in bitcoin on Steemit" src="https://steemit-production-imageproxy-upload.s3.amazonaws.com/DQmZiPLSZTc6iNeZL7R7cS2JW46z6wHvJ22kWFTLBptDUDH"&gt;
&lt;p style="text-align: right;"&gt;&lt;em&gt;(tommytwohats in bitcoin on Steemit)&lt;/em&gt;&lt;/p&gt;&lt;/p&gt;</content></entry><entry><title>on Bleu scores and transcription rates</title><link href="https://pluteski.github.io/speech-to-text/on-bleu-scores-and-transcription-rates.html" rel="alternate"></link><published>2017-04-30T00:00:00-07:00</published><updated>2017-04-30T00:00:00-07:00</updated><author><name>Mark Plutowski</name></author><id>tag:pluteski.github.io,2017-04-30:/speech-to-text/on-bleu-scores-and-transcription-rates.html</id><summary type="html">&lt;p&gt;&lt;em&gt;[Work in progress]&lt;/em&gt;&lt;/p&gt;
&lt;h1&gt;Reference audio vs noisy recordings&lt;/h1&gt;
&lt;p&gt;As expected, each service performs better on audio recorded in a
quiet setting.&lt;/p&gt;
&lt;p&gt;IBM and Google performed about the same over high quality
audio recorded in a quiet setting using a medium quality
microphone.&lt;/p&gt;
&lt;p&gt;IBM was better able to generate transcripts for …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;[Work in progress]&lt;/em&gt;&lt;/p&gt;
&lt;h1&gt;Reference audio vs noisy recordings&lt;/h1&gt;
&lt;p&gt;As expected, each service performs better on audio recorded in a
quiet setting.&lt;/p&gt;
&lt;p&gt;IBM and Google performed about the same over high quality
audio recorded in a quiet setting using a medium quality
microphone.&lt;/p&gt;
&lt;p&gt;IBM was better able to generate transcripts for
my lower quality recordings obtained in noisy settings.&lt;/p&gt;
&lt;p&gt;This could be due to a custom (which they refer to as a "narrowband")
setting that IBM
provides that is specifically provided to accommodate
low-bitrate recordings. That setting also tends to generate longer
transcripts for higher bitrate recordings that are especially noisy.&lt;/p&gt;
&lt;p&gt;It may also have to do with the encoding used. I needed to transcode
every audio file to the encoding required by Google's service, and it
is possible that this transcoding step could be tuned to give better accuracy.
I haven't attempted any rigorous tuning of either service at this stage of
my experiments.&lt;/p&gt;
&lt;p&gt;This is a work in progress.  I am hoping to do a direct file-by-file
comparison of the two services once I am confident that I am
configuring my settings to use Google's api to the best of its capability.
However I have obtained some cursory comparisons on the current results.&lt;/p&gt;
&lt;h5&gt;Processing seconds per minute : about the same&lt;/h5&gt;
&lt;p&gt;IBM and Google process the audio in about the same amount of time.
Google is somewhat faster at the actual transcription processing.
However, Google requires that the file be
uploaded to Google Storage first.&lt;/p&gt;
&lt;h5&gt;IBM Transcribed/Processed : 7281/8415&lt;/h5&gt;
&lt;p&gt;IBM was able to transcribe 87% of the files submitted.&lt;/p&gt;
&lt;h5&gt;Google Transcribed/Processed : 3521/8415&lt;/h5&gt;
&lt;p&gt;The transcription rate (42%) was lower for two main reasons.&lt;/p&gt;
&lt;p&gt;The first was a file size limit.
Files larger than ~80MB require a prior arrangement,
whereas IBM was able to process files of all sizes submitted to the service.&lt;/p&gt;
&lt;p&gt;The second main reason is that IBM has a custom setting for
low-bit rate recordings. Google failed to generate a transcript
for many files that were below the file size limit.&lt;/p&gt;
&lt;h5&gt;Transcript words per minute of audio (IBM/Google) : 102.0/9.8&lt;/h5&gt;
&lt;p&gt;Google's api doesn't have a setting for handling low-bitrate
recordings similar to IBM's narrowband setting.
The number of transcript
words generated per minute of audio was much lower for Google
even after adjusting for transcription rate.&lt;/p&gt;
&lt;h1&gt;Comparison on reference documents&lt;/h1&gt;
&lt;p&gt;The following comparisons were made over 245 reference documents.
The reference transcripts were transcribed using a speech-to-text
transcription software that was trained to my voice,
in a quiet environment, using a hand-held medium quality wired microphone.
Most of the errors were manually corrected.&lt;/p&gt;
&lt;p&gt;Google generated a transcript for 210 out of the 245 reference documents (86%),
and IBM generated a transcript for 243 of the 245 (99%).
The Bleu scores over these reference documents are fairly comparable,
with IBM performing slightly better.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Bleu score deciles" src="https://github.com/pluteski/speech-to-text/blob/master/images/bleu_score_deciles.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;When measured using Ratcliff-Obershelp similarity,
Google fares slightly better across the board.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ratcliff score deciles" src="https://github.com/pluteski/speech-to-text/blob/master/images/ratcliff_score_deciles.png?raw=true"&gt;&lt;/p&gt;
&lt;h2&gt;Comparison over all audio&lt;/h2&gt;
&lt;p&gt;This section analyzes the number of transcripts
that are generated, and the number of words per transcript.&lt;/p&gt;
&lt;p&gt;This comparison was over 8,415 audio files that were submitted to each
service.&lt;/p&gt;
&lt;p&gt;Marked differences between IBM Watson and Google transcription arise
when comparing transcription rates and number of words generated when run
on audio collected out in the wild. Of 8,415 such audio, IBM generated
transcripts for 7,227, while Google was able
to generate a transcript for 3,521.&lt;/p&gt;
&lt;h2&gt;Total Word Counts&lt;/h2&gt;
&lt;p&gt;Out of 8,415 audio files attempted, Google generated 3,521 transcripts.
Those 3,521 transcripts contain total of 485,334 words,
an average of 137 words per transcript.&lt;/p&gt;
&lt;p&gt;IBM Watson generated 7,227 transcripts, extracting
9,511,743 words out of those transcripts.
This gives an average of 1,316 words per transcript.&lt;/p&gt;
&lt;h2&gt;Word Count Deciles&lt;/h2&gt;
&lt;p&gt;Many of these transcripts that Google failed to generate were simply due
to the file size exceeding quota.&lt;/p&gt;
&lt;p&gt;However Google also failed to generate any transcript words
for many other files that did not exceed the file size quota.
It also generated a much lower word count per transcript for
audio that was from a noisy or low bit rate recording.&lt;/p&gt;
&lt;p&gt;One way to illustrate this is by examining the word count deciles over the
transcripts that were successfully generated.&lt;/p&gt;
&lt;p&gt;The following table gives the word counts deciles over the
transcripts generated by each service.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;API&lt;/th&gt;
&lt;th&gt;min&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;th&gt;max&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Google&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;58&lt;/td&gt;
&lt;td&gt;459&lt;/td&gt;
&lt;td&gt;4892&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IBM&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;278&lt;/td&gt;
&lt;td&gt;501&lt;/td&gt;
&lt;td&gt;698&lt;/td&gt;
&lt;td&gt;916&lt;/td&gt;
&lt;td&gt;1137&lt;/td&gt;
&lt;td&gt;1409&lt;/td&gt;
&lt;td&gt;1722&lt;/td&gt;
&lt;td&gt;2080&lt;/td&gt;
&lt;td&gt;2450&lt;/td&gt;
&lt;td&gt;8490&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img alt="Word count deciles" src="https://github.com/pluteski/speech-to-text/blob/master/images/word_count_deciles.png?raw=true"&gt;&lt;/p&gt;</content></entry><entry><title>on batch processing audio speech to text</title><link href="https://pluteski.github.io/speech-to-text/on-batch-processing-audio-speech-to-text.html" rel="alternate"></link><published>2017-04-22T00:00:00-07:00</published><updated>2017-04-22T00:00:00-07:00</updated><author><name>Mark Plutowski</name></author><id>tag:pluteski.github.io,2017-04-22:/speech-to-text/on-batch-processing-audio-speech-to-text.html</id><summary type="html">&lt;h1&gt;How not to wreck a nice beach&lt;/h1&gt;
&lt;p&gt;&lt;img alt="Quora" src="https://qph.ec.quoracdn.net/main-qimg-609e5b0b3c91845ab81f0d4448df864f-c"&gt;
&lt;p style="text-align: right;"&gt;&lt;em&gt;(Quora)&lt;/em&gt;&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;People nowadays commonly use speech-to-text tools for myriad uses,
including writing text messages, entering todo lists,
and writing emails.  However, increasing numbers of enterprising
sorts including just plain folk are starting to warm up to
using dictation software for writing longer form …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;How not to wreck a nice beach&lt;/h1&gt;
&lt;p&gt;&lt;img alt="Quora" src="https://qph.ec.quoracdn.net/main-qimg-609e5b0b3c91845ab81f0d4448df864f-c"&gt;
&lt;p style="text-align: right;"&gt;&lt;em&gt;(Quora)&lt;/em&gt;&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;People nowadays commonly use speech-to-text tools for myriad uses,
including writing text messages, entering todo lists,
and writing emails.  However, increasing numbers of enterprising
sorts including just plain folk are starting to warm up to
using dictation software for writing longer form content
such as journals and even novels.&lt;/p&gt;
&lt;p&gt;Dictation software has been generally available for many years,
in the form of
commercial off the shelf application software, text editors in desktop
operating systems, notepads in tablet devices,
email clients in mobile phones,
and more recently in cloud-based document editors, available
to the user from any device that can run a web browser.&lt;/p&gt;
&lt;p&gt;But what if you have a large-ish number of audio files?
It turns out there isn't a comparably inexpensive and easy way
to handle even modestly sized
batches of files using those just aforementioned tools.
There do exist commercially available alternatives, which will gladly
take those batches of audio files off your hands and return
high quality text. But these are still rather
human labor intensive, which means that they remain expensive.&lt;/p&gt;
&lt;p&gt;The typical going rate is about a $1/minute, although with a bit
of coding effort (typically meaning slicing files into uniform size work units,
farming said work units out to a distributed online on-demand work force
such as mechanical turk, then resplicing the files back together)
apparently can bring the price down to somewhat, say 70 cents/minute.
(The slicing and resplicing is needed for quality assurance as well
as to reduce privacy and security issues.)
That 70 cents a minute is still rather unaffordable to the typical consumer who is a tad long-winded,
much less the odd sort who just happens to have a boat load of such files.
Not to mention the concerns related to having unknown on-demand workers
hearing an early preview of your memoirs
(cf. &lt;a href="https://www.wired.com/2016/04/long-form-voice-transcription/"&gt;Long Form Voice Transcription, Wired, April 2016&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Fortunately, an affordable alternative is emerging, that is,
if you're willing to write a bit of code.
All of the hyperscale cloud api providers have made their deep
learning speech-to-text models available for a small fraction of the
dollar cost of the aforementioned human-corrected transcription services,
and two of them are now appealing enough for use by individuals
or nonprofits to apply to long form content.
Currently you can get cloud api based speech transcription
for
&lt;a href="https://github.com/pluteski/speech-to-text/wiki/Cloud-API-Pricing"&gt;prices ranging from 2 to 2.4 cents per minute&lt;/a&gt;,
with an additional allowance free on a monthly basis.
Moreover, they effortlessly handle large batches in a single bound.
You upload your files to the cloud, offloading the
processing from your laptop. Whereas your desktop software or even
embedded operating system software would slow your entire machine down
to a crawl during this process
you could be processing batches of files in parallel
up in the cloud.&lt;/p&gt;
&lt;p&gt;Although there is still that small issue of having to write some code
in order to get this amazing cost and time reduction.
&lt;a href="https://github.com/pluteski/speech-to-text"&gt;I went and wrote said code.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So this amazing capability is generally available
at a reasonable price
to anyone with a laptop, internet connection,
and credit card with an embarrassingly low limit.
But it isn't all peaches and chocolate just yet.
There is a still a ways to go before these APIs are reliably useful
for converting conversational audio recorded in the wild using
mobile phones amidst the usual ambient sounds.
Nonetheless, the results are tantalizingly good enough
and the pricing dramatically low enough that the time has come to dip
my metaphorical toes in the water.&lt;/p&gt;
&lt;p&gt;I intend to write more about my experiences using these
powerful cloud APIs, so stay tuned if this is of interest
to you.  Although I myself tend to cringe when the blogger ends
their post by promising an enticing followup, which invariably never
comes.  But I really do sincerely intend to write more on this.
Of course, that's what they would all say, I'm sure.&lt;/p&gt;
&lt;p&gt;I have already compiled a backlog of interesting results just waiting
to be converted into
intriguing visuals and wrapped in witty yet insightful prose. Promise.&lt;/p&gt;
&lt;p&gt;Actually you don't need to wait until then, as
&lt;a href="https://github.com/pluteski/speech-to-text/wiki/Background"&gt;here is some additional background on the project&lt;/a&gt;.
Here are the
&lt;a href="https://github.com/pluteski/speech-to-text/wiki/Findings"&gt;results of a preliminary comparison between IBM and Google&lt;/a&gt;.
Here are
&lt;a href="https://github.com/pluteski/speech-to-text/wiki/Comparison-over-reference-documents"&gt;results made over reference documents&lt;/a&gt;.
And here are some
&lt;a href="https://github.com/pluteski/speech-to-text/wiki/Comparison-of-transcript-word-count"&gt;results made over audio collected in the wild&lt;/a&gt;.
There are several conclusions that can be drawn from these findings,
which will be the subject of a future post.&lt;/p&gt;
&lt;p&gt;In closing, my findings indicate to me that I should be able to
extract useful text from my recordings,
albeit probably not for the purpose of transcribing meeting minutes.
The transcript text generated by the cloud API is largely recognizable to me, although
that is often in the same way as my handwriting is recognizable to me
because I vaguely remember generating it.
The error rates are still too high
for these transcripts to allow me to reliably parse meaningful
whole sentences much less serve as document of record.&lt;/p&gt;
&lt;p&gt;These cloud APIs do not have the same quality as
do personalized devices. Voicemail transcripts provided by my
phone are subjectively better than what I'm seeing with these cloud APIs.
One reason is that because the cloud APIs cannot be
trained on my voice, although IBM does allow specifying
a &lt;a href="https://www.ibm.com/watson/developercloud/doc/speech-to-text/custom.html#addWords"&gt;custom language model&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The accuracy being provided
by the cloud APIs may be suitable for
indexing audio files to make them more easily searchable
(although phonemes might turn out to be the better data unit
for indexing audio than words,
&lt;a href="https://twimlai.com/from-particle-physics-to-audio-ai-with-scott-stephenson/"&gt;e.g., Deepgram&lt;/a&gt;).
I fully expect the current results to
be adequately informative to support meaningful textmining and trend
analysis.&lt;/p&gt;
&lt;p&gt;Upon reviewing the current batch of results it is fascinating
to me how a badly transcribed
sequence of just a few words in a row can turn large swaths of text
into complete gibberish. If you tell a linguist you intend to
&lt;em&gt;recognize speech&lt;/em&gt; they might bid you luck, but an evesdropping
surfer taking that to mean instead
that you intend to &lt;em&gt;wreck a nice beach&lt;/em&gt; might wish you harm.&lt;/p&gt;
&lt;p&gt;We humans are pretty amazing in our ability
to understand each other's muffled speech despite the
poor connection and background noise
much less the tinnitus and what have you.&lt;/p&gt;
&lt;p&gt;Nonetheless, I foresee a not too distant future where
it will be possible to cheaply attain human level accuracy
transcription of speaker-independent long-form speech
from noisy recordings obtained using mobile devices. Culminating in,
as &lt;a href="https://www.wired.com/2016/04/long-form-voice-transcription/"&gt;Wired's Jesse Jarnow&lt;/a&gt; puts it,
'bizarre kinds of unforeseen societal change'.&lt;/p&gt;
&lt;p&gt;Until then, court stenographers shouldn't have much to worry about.&lt;/p&gt;
&lt;div id="disqus_thread"&gt;&lt;/div&gt;

&lt;script&gt;

var disqus_config = function () {
this.page.url = "https://pluteski.github.io/speech-to-text/"
this.page.identifier = "pluteski-github-io-speech-to-text"
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://pluteski-github-io-speech-to-text.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript"&gt;comments.&lt;/a&gt;&lt;/noscript&gt;

&lt;h6&gt;fin&lt;/h6&gt;</content></entry></feed>